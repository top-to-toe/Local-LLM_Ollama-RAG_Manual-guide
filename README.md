# Local-LLM_Ollama-RAG_Manual-guide
실무자를 위한 로컬 LLM 환경 구축 및 활용 매뉴얼. Ollama와 Docker를 활용하여 오프라인에서도 동작하는 문서 기반 질의응답(RAG) 시스템을 구축하는 가이드입니다. 복잡한 기술 용어를 배제하고, 실제 인턴십 과정의 시행착오와 문제 해결 과정을 담은 실무형 가이드입니다. Ollama 기반의 로컬 LLM과 RAG 기술을 활용하여 오프라인에서 안전하게 문서를 다루는 방법을 설명합니다.

A step-by-step guide for building a local LLM environment using Ollama and Docker. This repository provides a practical manual for implementing Retrieval-Augmented Generation (RAG) capabilities for offline use.

## 매뉴얼 주요 내용 요약

### 1. 로컬 AI 환경 구축 및 실행
-   **Docker**를 활용한 **Ollama 및 Open WebUI** 설치 방법.
-   설치 과정에서 발생 가능한 **일반적인 오류 해결책** 제시.

### 2. LLM 모델 관리 및 활용
-   **모델 다운로드 및 Open WebUI 연동** 절차.
-   컨테이너 재부팅 시 발생하는 **모델 비활성화 문제 해결 방안** 제시.
-   **모델별 특성 분석**을 통한 최적의 모델 선정 가이드 제공.
-   **모델 양자화(Quantization)**를 통한 **성능 및 메모리 최적화 기법** 설명.
-   Ollama 및 모델 **업데이트와 유지보수** 방법 정리.
