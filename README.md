# Local-LLM_Ollama-RAG_Manual-guide
실무자를 위한 로컬 LLM 환경 구축 및 활용 매뉴얼. Ollama와 Docker를 활용하여 오프라인에서도 동작하는 문서 기반 질의응답(RAG) 시스템을 구축하는 가이드입니다. 복잡한 기술 용어를 배제하고, 실제 인턴십 과정의 시행착오와 문제 해결 과정을 담은 실무형 가이드입니다. Ollama 기반의 로컬 LLM과 RAG 기술을 활용하여 오프라인에서 안전하게 문서를 다루는 방법을 설명합니다.
A step-by-step guide for building a local LLM environment using Ollama and Docker. This repository provides a practical manual for implementing Retrieval-Augmented Generation (RAG) capabilities for offline use.
