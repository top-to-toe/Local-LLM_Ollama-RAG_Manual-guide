# Local-LLM_Ollama-RAG_Manual-guide
실무자를 위한 로컬 LLM 환경 구축 및 활용 매뉴얼. Ollama와 Docker를 활용하여 오프라인에서도 동작하는 문서 기반 질의응답(RAG) 시스템을 구축하는 가이드입니다. 복잡한 기술 용어를 배제하고, 실제 인턴십 과정의 시행착오와 문제 해결 과정을 담은 실무형 가이드입니다. Ollama 기반의 로컬 LLM과 RAG 기술을 활용하여 오프라인에서 안전하게 문서를 다루는 방법을 설명합니다.

A step-by-step guide for building a local LLM environment using Ollama and Docker. This repository provides a practical manual for implementing Retrieval-Augmented Generation (RAG) capabilities for offline use.

---

## 1. 로컬 AI 환경 구축 및 실행

### 1.1. 시스템 요구사항 및 준비 사항
로컬 LLM 환경을 구축하기 위해 필요한 하드웨어 및 소프트웨어 요구사항을 확인합니다.
- 운영체제: Windows, macOS, Linux
- 하드웨어: 충분한 RAM(최소 8GB 권장), GPU(선택사항, VRAM 8GB 이상 권장)
- 소프트웨어: Docker Desktop 설치

### 1.2. Ollama 설치 및 기본 설정
Ollama는 로컬 환경에서 LLM을 쉽게 실행할 수 있도록 돕는 도구입니다. Docker를 사용하여 Ollama 컨테이너를 설치합니다.

### 1.3. Open WebUI 설치 및 접속
Open WebUI는 Ollama와 연동하여 웹 브라우저에서 편리하게 LLM을 사용할 수 있는 웹 인터페이스입니다.

### 1.4. 설치 오류 및 문제 해결
설치 과정에서 발생할 수 있는 주요 오류와 해결 방법을 다룹니다.

---

## 2. LLM 모델 관리 및 활용

로컬 환경 구축을 마쳤다면, 이제 LLM 모델을 직접 다운로드하고 관리하는 방법을 알아볼 차례입니다. 이 챕터는 모델을 효율적으로 사용하고, 발생할 수 있는 문제에 대처하는 실질적인 가이드를 제공합니다.

### 2.1. LLM 모델 다운로드 및 연동
Ollama는 `ollama pull` 명령어를 통해 다양한 모델을 간편하게 다운로드할 수 있습니다. 모델을 다운로드하려면 관리자 권한으로 명령 프롬프트를 실행해야 합니다.

**명령 프롬프트(CMD)를 관리자 권한으로 실행하는 방법**은 다음과 같습니다.
* **방법 1: 윈도우 검색창**
    1.  윈도우 작업 표시줄의 **검색창**에 `cmd` 또는 `명령 프롬프트`를 입력합니다.
    2.  검색 결과로 나온 `명령 프롬프트` 앱을 마우스 오른쪽 버튼으로 클릭합니다.
    3.  나타나는 메뉴에서 **`관리자 권한으로 실행`**을 클릭합니다.
* **방법 2: 시작 메뉴**
    1.  윈도우 **시작 버튼**을 마우스 오른쪽 버튼으로 클릭합니다.
    2.  메뉴에서 **`명령 프롬프트(관리자)`** 또는 **`Windows PowerShell(관리자)`**를 클릭합니다.

Ollama 컨테이너가 실행된 상태에서, 관리자 권한으로 실행된 명령 프롬프트에 아래 명령어를 입력하여 **`llama2`** 모델을 다운로드합니다. 이 명령어는 명확하게 모델을 다운로드하는 역할만 수행합니다.

* **실행 예시**:
    ```
    ollama pull llama2
    ```

`ollama pull` 명령어를 통해 모델을 다운로드하면, Ollama 컨테이너가 자동으로 해당 모델을 Open WebUI 컨테이너와 연결해줍니다. 이것이 바로 '연동' 과정입니다. 다운로드가 완료되면 Open WebUI 웹페이지의 모델 선택 목록에 **`llama2:latest`** 모델이 정상적으로 나타나는 것을 확인할 수 있습니다. 이제 해당 모델을 선택하면, LLM 모델과의 대화가 정상적으로 시작됩니다.

### 2.2. 컨테이너 재부팅 후 모델이 사라지는 현상
Docker 컨테이너를 재시작하거나 PC를 재부팅하면 이전에 다운로드했던 LLM 모델이 사라진 것처럼 보일 수 있습니다. 이는 컨테이너의 휘발성(Volatile) 특성 때문에 발생하는 문제입니다. 이 문제를 해결하기 위해 모델을 다시 다운로드할 필요는 없습니다.

이미 다운로드된 모델을 다시 사용하려면, 명령 프롬프트(CMD)에서 `ollama run` 명령어를 다시 입력하면 됩니다. Ollama는 해당 명령어를 입력했을 때 컨테이너 내에 모델이 있는지 먼저 확인하고, 이미 존재한다면 바로 실행하여 모델을 다시 사용할 수 있도록 합니다.

* **실행 예시**:
    ```
    ollama run llama2
    ```

### 2.3. 모델별 특성 및 활용법
Ollama는 다양한 LLM 모델을 제공하며, 각 모델은 고유한 특성을 가지고 있습니다. 사용 목적에 맞는 모델을 선택하는 것은 매우 중요합니다.

**일반적인 용도로 추천하는 모델**
* **`llama2`**: 가장 널리 사용되는 오픈소스 모델로, 대화나 문서 요약 등 광범위한 작업에 적합합니다.
* **`llama3`**: `llama2`의 후속 모델로, 더 복잡한 추론과 코딩 작업에서 뛰어난 성능을 보입니다.
* **`gemma`**: 구글에서 개발한 경량 모델로, 빠른 응답 속도가 중요한 환경에 적합합니다.

**특화된 용도에 맞는 모델**
* **`codellama`**: 코딩 작업에 특화되어 코드 생성 및 디버깅에 탁월합니다.
* **`phi3`**: 마이크로소프트의 소형 언어 모델로, 저사양 기기에서도 준수한 성능을 제공합니다.

### 2.4. 성능 최적화 (메모리, 속도)
로컬 환경에서 LLM 모델을 효율적으로 사용하려면 하드웨어 자원을 최적화해야 합니다. 특히 메모리(VRAM)와 응답 속도를 향상시키는 것이 중요합니다.

**메모리(VRAM) 최적화: 모델 양자화(Quantization)**
대부분의 LLM 모델은 큰 용량을 차지하므로 VRAM이 부족하면 실행이 어렵습니다. 이를 해결하기 위해 **'양자화(Quantization)'** 기술을 사용합니다. 모델의 정밀도를 낮춰 크기를 줄이는 기술로, 저사양 PC에서도 원활하게 작동하도록 돕습니다. Ollama는 다양한 양자화 버전을 제공하며, 아래와 같이 용도에 맞는 버전을 선택하여 다운로드할 수 있습니다.

* **활용 예시**:
    ```
    ollama run llama2:7b-q4_0
    ```
**속도 최적화: 모델 크기 선택**
모델의 크기는 응답 속도에 직접적인 영향을 미칩니다. 일반적으로 모델이 클수록 성능은 좋지만 응답 속도가 느려집니다.

* **빠른 속도가 중요할 때:** `gemma`, `phi`, `tinyllama` 등 **7B(70억 개) 이하**의 경량 모델을 추천합니다.
* **성능과 속도를 모두 고려할 때:** **`7B` 모델**을 사용하면 균형 잡힌 성능을 기대할 수 있습니다.
* **정확도와 성능이 가장 중요할 때:** GPU 메모리가 충분하다면 `13B` 이상의 더 큰 모델을 사용합니다.

### 2.5. 업데이트 및 유지보수
로컬 AI 환경을 안정적으로 운영하기 위해 Ollama와 LLM 모델을 주기적으로 업데이트하고 관리하는 방법을 알아봅니다.

* **Ollama 업데이트**
    Ollama 개발팀은 정기적으로 업데이트를 배포합니다. 아래 명령어를 사용해 현재 실행 중인 컨테이너를 제거하고 최신 버전을 다시 설치하여 업데이트할 수 있습니다.
    ```
    docker stop ollama
    docker rm ollama
    docker run -d --name ollama -v ollama:/root/.ollama -p 127.0.0.1:11434:11434 --restart always ollama/ollama
    ```
* **LLM 모델 업데이트**
    다운로드한 LLM 모델의 최신 버전이 나왔을 경우, `ollama pull` 명령어를 통해 업데이트할 수 있습니다.
    ```
    ollama pull [모델명]
    ```
