# Local-LLM_Ollama-RAG_Manual-guide
실무자를 위한 로컬 LLM 환경 구축 및 활용 매뉴얼. Ollama와 Docker를 활용하여 오프라인에서도 동작하는 문서 기반 질의응답(RAG) 시스템을 구축하는 가이드입니다. 복잡한 기술 용어를 배제하고, 실제 인턴십 과정의 시행착오와 문제 해결 과정을 담은 실무형 가이드입니다. Ollama 기반의 로컬 LLM과 RAG 기술을 활용하여 오프라인에서 안전하게 문서를 다루는 방법을 설명합니다.

A step-by-step guide for building a local LLM environment using Ollama and Docker. This repository provides a practical manual for implementing Retrieval-Augmented Generation (RAG) capabilities for offline use.

---

## 1. 로컬 AI 환경 구축 및 실행

### 1.1. 시스템 요구사항 및 준비 사항
로컬 LLM 환경을 구축하기 위해 필요한 하드웨어 및 소프트웨어 요구사항을 확인합니다.
- 운영체제: Windows, macOS, Linux
- 하드웨어: 충분한 RAM(최소 8GB 권장), GPU(선택사항, VRAM 8GB 이상 권장)
- 소프트웨어: Docker Desktop 설치

### 1.2. Ollama 설치 및 기본 설정
Ollama는 로컬 환경에서 LLM을 쉽게 실행할 수 있도록 돕는 도구입니다. Docker를 사용하여 Ollama 컨테이너를 설치합니다.

### 1.3. Open WebUI 설치 및 접속
Open WebUI는 Ollama와 연동하여 웹 브라우저에서 편리하게 LLM을 사용할 수 있는 웹 인터페이스입니다.

### 1.4. 설치 오류 및 문제 해결
설치 과정에서 발생할 수 있는 주요 오류와 해결 방법을 다룹니다.

---

## 2. LLM 모델 관리 및 활용

### 2.1. LLM 모델 다운로드 및 연동
모델을 다운로드하고 설치하기 위해서는 **관리자 권한으로 명령 프롬프트**를 실행해야 합니다. 아래 방법 중 하나를 선택하여 명령 프롬프트를 실행해 주세요.

* **방법 1: 윈도우 검색창**
    1.  윈도우 작업 표시줄의 **검색창**에 `cmd` 또는 `명령 프롬프트`를 입력합니다.
    2.  검색 결과로 나온 `명령 프롬프트` 앱을 마우스 오른쪽 버튼으로 클릭합니다.
    3.  나타나는 메뉴에서 **`관리자 권한으로 실행`**을 클릭합니다.
* **방법 2: 시작 메뉴**
    1.  윈도우 **시작 버튼**을 마우스 오른쪽 버튼으로 클릭합니다.
    2.  메뉴에서 **`명령 프롬프트(관리자)`** 또는 **`Windows PowerShell(관리자)`**를 클릭합니다.

Ollama 컨테이너가 실행된 상태에서, 관리자 권한으로 실행된 명령 프롬프트에 아래 명령어를 입력하여 **`llama2`** 모델을 다운로드합니다. 이 명령어는 명확하게 모델을 다운로드하는 역할만 수행합니다.

ollama pull llama2

![llama2 모델 다운로드 진행](image_271be6.png)

`ollama pull` 명령어를 통해 모델을 다운로드하면, Ollama 컨테이너가 자동으로 해당 모델을 Open WebUI 컨테이너와 연결해줍니다. 이것이 바로 '연동' 과정입니다. 다운로드가 완료되면 Open WebUI 웹페이지의 모델 선택 목록에 **`llama2:latest`** 모델이 정상적으로 나타나는 것을 확인할 수 있습니다. 이제 해당 모델을 선택하면, LLM 모델과의 대화가 정상적으로 시작됩니다.

![llama2 모델 연동 확인](image_e2f429.png)

### 2.2. 컨테이너 재부팅 후 모델이 사라지는 현상
* **문제 발생 원인**: Docker 컨테이너는 본래 휘발성(Volatile)의 특성을 가지고 있습니다. 컨테이너를 재시작하거나 PC를 재부팅하면 컨테이너 내부에 저장된 데이터가 초기화될 수 있음을 의미합니다. 이 때문에 이전에 다운로드했던 LLM 모델이 사라진 것처럼 보일 수 있습니다.
* **해결 방법**: 모델을 다시 다운로드하는 것이 아니라, 명령 프롬프트(CMD)에서 `ollama run [모델명]` 명령어를 다시 입력하면 됩니다. Ollama는 해당 명령어를 입력했을 때, 컨테이너 내에 모델이 있는지 먼저 확인하고 없다면 자동으로 다운로드하며, 이미 존재한다면 바로 실행하여 모델을 다시 사용할 수 있도록 합니다.

* **실행 예시**:
    ```
    ollama run llama2
    ```

### 2.3. 모델별 특성 및 활용법
Ollama에서 다운로드할 수 있는 LLM 모델들은 각기 다른 특성을 가지고 있습니다. 매개변수(parameter)의 수가 모델의 크기와 성능에 영향을 미치며, 사용 목적에 맞는 모델을 선택하는 것이 중요합니다.

* **`llama2`**: 가장 널리 사용되는 오픈소스 모델입니다. 7B, 13B, 70B 등 다양한 크기가 있으며, 일반적인 대화나 문서 요약 등 광범위한 작업에 적합합니다.
* **`llama3`**: `llama2`의 후속 모델로, 성능이 크게 향상되었습니다. 더 복잡한 추론과 코딩 작업에서 뛰어난 성능을 보입니다.
* **`gemma`**: 구글에서 개발한 경량 모델입니다. `7B`와 `2B` 버전이 있으며, 상대적으로 작은 크기 덕분에 빠른 응답 속도가 중요한 환경에서 유용합니다.
* **`codellama`**: 코딩 작업에 특화된 모델입니다. Python, C++, Java 등 다양한 프로그래밍 언어의 코드 생성, 디버깅, 주석 달기 등에 탁월한 성능을 발휘합니다.
* **`phi3`**: 마이크로소프트에서 개발한 소형 언어 모델입니다. 가벼운 크기에도 불구하고 준수한 성능을 보여주어, 저사양 기기나 교육용으로 사용하기에 좋습니다.

### 2.4. 성능 최적화 (메모리, 속도)
로컬 환경에서 LLM 모델을 효율적으로 사용하기 위해서는 하드웨어 자원(특히 메모리와 속도)을 최적화하는 것이 중요합니다. 아래 내용을 통해 모델 성능을 향상시키는 방법을 알아보겠습니다.

* **메모리(VRAM) 최적화: 모델 양자화(Quantization)**
    * **양자화란?** 모델의 정밀도를 낮춰 모델의 크기를 줄이는 기술입니다. 용량과 메모리 사용량을 크게 줄여 저사양 PC에서도 원활하게 작동합니다.
    * **활용법:** Ollama는 다양한 양자화 버전을 제공합니다. 모델 다운로드 시 용도에 맞는 버전을 선택하여 사용하면 됩니다. 예를 들어, `ollama run llama2:7b-q4_0` 명령어를 사용하면 4-bit로 양자화된 모델을 다운로드할 수 있습니다.
* **속도 최적화: 모델 크기와 선택**
    * 모델의 크기는 응답 속도에 직접적인 영향을 미칩니다. 일반적으로 모델의 크기(매개변수 수)가 클수록 성능은 좋아지지만, 응답 속도는 느려집니다.
    * **빠른 속도가 중요할 때:** `gemma`, `phi`, `tinyllama` 등 **7B(70억 개) 이하**의 경량 모델을 사용하는 것을 추천합니다.
    * **성능과 속도를 모두 고려할 때:** `llama2:7b` 또는 `gemma:7b`와 같이 **7B** 모델을 사용하면 균형 잡힌 성능을 기대할 수 있습니다.
    * **정확도와 성능이 가장 중요할 때:** `llama2:13b` 또는 `codellama:34b`와 같이 더 큰 모델을 사용합니다. GPU 메모리가 충분하다면 더 정확하고 풍부한 답변을 제공합니다.

### 2.5. 업데이트 및 유지보수
로컬 AI 환경을 안정적으로 운영하기 위해서는 Ollama와 LLM 모델을 주기적으로 업데이트하고 관리하는 것이 중요합니다.

* **Ollama 업데이트**
    * Ollama 개발팀은 버그 수정 및 성능 개선을 위해 정기적으로 업데이트를 배포합니다. Ollama를 업데이트하려면, 아래 명령어를 사용해 현재 실행 중인 Ollama 컨테이너를 제거하고 최신 버전의 컨테이너를 다시 다운로드해야 합니다.
    ```
    docker stop ollama
    docker rm ollama
    docker run -d --name ollama -v ollama:/root/.ollama -p 127.0.0.1:11434:11434 --restart always ollama/ollama
    ```
* **LLM 모델 업데이트**
    * 다운로드한 LLM 모델의 최신 버전이 나왔을 경우, 기존 모델을 삭제하고 새 버전을 다운로드하거나 `ollama pull` 명령어를 통해 업데이트할 수 있습니다.
    ```
    ollama pull [모델명]
    ```
